{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c426e006-1b6e-40e9-9e6c-7619b45b13c9",
   "metadata": {},
   "source": [
    "# Attention Transformer NN for King Heritage Data\n",
    "\n",
    "## Run 17\n",
    "\n",
    "\n",
    "`Test_35*-Test_37*`\n",
    "\n",
    "Same as Train_16, with the 3 variations, except MUCH smaller embedding dimension; 200 -> 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605e3fb-9fe6-4741-93b4-9097835e17be",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6d0319-88d9-4841-9ac9-db6e1f67b8e0",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-26T06:41:27.867765Z",
     "iopub.status.busy": "2024-11-26T06:41:27.867414Z",
     "iopub.status.idle": "2024-11-26T06:41:27.871962Z",
     "shell.execute_reply": "2024-11-26T06:41:27.870869Z",
     "shell.execute_reply.started": "2024-11-26T06:41:27.867735Z"
    },
    "gather": {
     "logged": 1722536559916
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#!conda activate jupyter_env\n",
    "#!pip install -r \"../requirements.txt\"\n",
    "# !pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf26f2f-02b7-4c6b-b2a8-8f6b7ac73c8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T05:03:03.834784Z",
     "iopub.status.busy": "2024-12-27T05:03:03.834288Z",
     "iopub.status.idle": "2024-12-27T05:03:03.958875Z",
     "shell.execute_reply": "2024-12-27T05:03:03.958318Z",
     "shell.execute_reply.started": "2024-12-27T05:03:03.834748Z"
    },
    "gather": {
     "logged": 1723055177809
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Import meta setup\n",
    "\n",
    "# In order to force reload any changes done to the models package files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Allow import from our custom lib python files\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('../'))\n",
    "module_path = os.path.abspath(os.path.join('../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbc08f-25e6-4a1d-ad34-251aa798954b",
   "metadata": {},
   "source": [
    "### Fix for working in TLJH context\n",
    "\n",
    "Issue with multithreading spawning with the jupyter hub setup (The Littlest Jupyter Hub on Paperspace) and working with torch, and with the dataloader multithread loading. See the following for issue discussion and solution: \n",
    "\n",
    "https://github.com/pytorch/pytorch/issues/40403#issuecomment-1704178443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d0c594-fc79-481f-bc32-b403253b1f43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T05:03:07.082242Z",
     "iopub.status.busy": "2024-12-27T05:03:07.081889Z",
     "iopub.status.idle": "2024-12-27T05:03:09.948969Z",
     "shell.execute_reply": "2024-12-27T05:03:09.948013Z",
     "shell.execute_reply.started": "2024-12-27T05:03:07.082214Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp \n",
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b57708-51fd-4e48-9876-b88e6db99a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T05:03:10.480810Z",
     "iopub.status.busy": "2024-12-27T05:03:10.480372Z",
     "iopub.status.idle": "2024-12-27T05:03:21.707425Z",
     "shell.execute_reply": "2024-12-27T05:03:21.706703Z",
     "shell.execute_reply.started": "2024-12-27T05:03:10.480781Z"
    },
    "gather": {
     "logged": 1723055198109
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-26 21:03:17.739573: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/bjonnalagadda/.pyenv/versions/3.10.0/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "from lib.params import * # device, use_cuda, Checkpoint, various saving strs\n",
    "from lib.datasets import TokenizedKingDataset, TokenizedCollateFn\n",
    "from lib.models import TokenizedInputTransformer\n",
    "from lib.saveload import *\n",
    "from lib.training import train_model_tokenized, tokenized_masked_loss\n",
    "import lib.notebook_utils as custom_info\n",
    "\n",
    "import dill\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75144438",
   "metadata": {},
   "source": [
    "### Debug Machine Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2fb97d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T05:03:21.710844Z",
     "iopub.status.busy": "2024-12-27T05:03:21.709329Z",
     "iopub.status.idle": "2024-12-27T05:03:22.814523Z",
     "shell.execute_reply": "2024-12-27T05:03:22.813396Z",
     "shell.execute_reply.started": "2024-12-27T05:03:21.710812Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python executable: /Users/bjonnalagadda/.pyenv/versions/3.10.0/bin/python3.10\t3.10.0 3.10.0 (default, Apr 18 2023, 17:50:49) [Clang 12.0.0 (clang-1200.0.32.29)]\n",
      "Current Directory: /Users/bjonnalagadda/Library/CloudStorage/OneDrive-UCB-O365/Work/Ecotone/Internship2024/Notebooks\n",
      "==================== Imported Packages ====================\n",
      "sys==Python BuiltIn\n",
      "dill==0.3.6\n",
      "numpy==1.24.4\n",
      "pandas==2.1.4\n",
      "torch==2.0.1\n",
      "os==unknown\n",
      "json==unknown\n",
      "datetime==unknown\n",
      "lib==unknown\n",
      "======================================== System ========================================\n",
      "System: Darwin\n",
      "Node Name: svasudevanmbp\n",
      "Release: 23.6.0\n",
      "Version: Darwin Kernel Version 23.6.0: Fri Jul  5 17:54:20 PDT 2024; root:xnu-10063.141.1~2/RELEASE_X86_64\n",
      "Machine: x86_64\n",
      "Processor: i386\n",
      "======================================== CPU ========================================\n",
      "Physical cores: 6\n",
      "Total cores: 12\n",
      "Max Frequency: 2600.00Mhz\n",
      "Min Frequency: 2600.00Mhz\n",
      "Current Frequency: 2600.00Mhz\n",
      "CPU Usage Per Core:\n",
      "Core 0: 63.6%\n",
      "Core 1: 0.0%\n",
      "Core 2: 64.6%\n",
      "Core 3: 2.0%\n",
      "Core 4: 51.5%\n",
      "Core 5: 0.0%\n",
      "Core 6: 49.0%\n",
      "Core 7: 1.0%\n",
      "Core 8: 42.6%\n",
      "Core 9: 1.0%\n",
      "Core 10: 35.4%\n",
      "Core 11: 0.0%\n",
      "Total CPU Usage: 28.0%\n",
      "======================================== Memory ========================================\n",
      "Total: 32.00GB\n",
      "Available: 18.69GB\n",
      "Used: 11.19GB\n",
      "Percentage: 41.6%\n",
      "==================== SWAP ====================\n",
      "Total: 2.00GB\n",
      "Free: 1.50GB\n",
      "Used: 509.00MB\n",
      "Percentage: 24.9%\n",
      "======================================== Disk ========================================\n",
      "Partitions and Usage:\n",
      "=== Device: /dev/disk1s4s1 ===\n",
      "  Mountpoint: /\n",
      "  File system type: apfs\n",
      "  Total Size: 931.55GB\n",
      "  Used: 9.56GB\n",
      "  Free: 127.68GB\n",
      "  Percentage: 7.0%\n",
      "=== Device: /dev/disk1s2 ===\n",
      "  Mountpoint: /System/Volumes/Preboot\n",
      "  File system type: apfs\n",
      "  Total Size: 931.55GB\n",
      "  Used: 1.98GB\n",
      "  Free: 127.68GB\n",
      "  Percentage: 1.5%\n",
      "=== Device: /dev/disk1s6 ===\n",
      "  Mountpoint: /System/Volumes/VM\n",
      "  File system type: apfs\n",
      "  Total Size: 931.55GB\n",
      "  Used: 2.00GB\n",
      "  Free: 127.68GB\n",
      "  Percentage: 1.5%\n",
      "=== Device: /dev/disk1s5 ===\n",
      "  Mountpoint: /System/Volumes/Update\n",
      "  File system type: apfs\n",
      "  Total Size: 931.55GB\n",
      "  Used: 620.00KB\n",
      "  Free: 127.68GB\n",
      "  Percentage: 0.0%\n",
      "=== Device: /dev/disk1s1 ===\n",
      "  Mountpoint: /System/Volumes/Data\n",
      "  File system type: apfs\n",
      "  Total Size: 931.55GB\n",
      "  Used: 788.97GB\n",
      "  Free: 127.68GB\n",
      "  Percentage: 86.1%\n",
      "Total read: 4.41TB\n",
      "Total write: 4.78TB\n",
      "======================================== Network ========================================\n",
      "=== Interface: lo0 ===\n",
      "  IP Address: 127.0.0.1\n",
      "  Netmask: 255.0.0.0\n",
      "  Broadcast IP: None\n",
      "=== Interface: lo0 ===\n",
      "=== Interface: lo0 ===\n",
      "=== Interface: en0 ===\n",
      "  IP Address: 192.168.1.180\n",
      "  Netmask: 255.255.255.0\n",
      "  Broadcast IP: 192.168.1.255\n",
      "=== Interface: en0 ===\n",
      "=== Interface: en0 ===\n",
      "=== Interface: en0 ===\n",
      "=== Interface: en0 ===\n",
      "=== Interface: en0 ===\n",
      "=== Interface: ap1 ===\n",
      "=== Interface: awdl0 ===\n",
      "=== Interface: awdl0 ===\n",
      "=== Interface: llw0 ===\n",
      "=== Interface: llw0 ===\n",
      "=== Interface: en2 ===\n",
      "=== Interface: en3 ===\n",
      "=== Interface: en1 ===\n",
      "=== Interface: en4 ===\n",
      "=== Interface: bridge0 ===\n",
      "=== Interface: en8 ===\n",
      "=== Interface: en8 ===\n",
      "=== Interface: utun0 ===\n",
      "=== Interface: utun1 ===\n",
      "=== Interface: utun2 ===\n",
      "=== Interface: utun3 ===\n",
      "Total Bytes Sent: 6.16GB\n",
      "Total Bytes Received: 6.55GB\n",
      "======================================== GPU ========================================\n",
      "id    name    load    free memory    used memory    total memory    temperature    uuid\n",
      "----  ------  ------  -------------  -------------  --------------  -------------  ------\n"
     ]
    }
   ],
   "source": [
    "custom_info.print_python_info()\n",
    "custom_info.print_imports(globals())\n",
    "custom_info.print_machine_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480465ba-0bae-40f4-b9cb-cc7c05db14c0",
   "metadata": {},
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3a54ff7-0a8e-4794-883a-18df67ab452e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-27T05:03:22.816998Z",
     "iopub.status.busy": "2024-12-27T05:03:22.816444Z",
     "iopub.status.idle": "2024-12-27T05:03:22.889682Z",
     "shell.execute_reply": "2024-12-27T05:03:22.888657Z",
     "shell.execute_reply.started": "2024-12-27T05:03:22.816960Z"
    },
    "gather": {
     "logged": 1723055216785
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "runname = \"test17\"\n",
    "machine = \"Paperspace\"\n",
    "datapath = \"../Data/king_matrix.csv\"\n",
    "outdir = os.path.join(\"../Output/Runs/\", runname)\n",
    "tensorboard_dir = \"../Output/Tensorboard\"\n",
    "SEED = 42\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "if not os.path.exists(tensorboard_dir):\n",
    "    os.mkdir(tensorboard_dir)\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f9495-6e07-4030-a22f-598f212ed9fe",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04af740b-96ff-486c-bdc6-caad82e053ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T05:15:07.979039Z",
     "iopub.status.busy": "2024-12-27T05:15:07.978424Z",
     "iopub.status.idle": "2024-12-27T05:15:11.795474Z",
     "shell.execute_reply": "2024-12-27T05:15:11.794441Z",
     "shell.execute_reply.started": "2024-12-27T05:15:07.979010Z"
    },
    "gather": {
     "logged": 1723055389380
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All (shuffled) inds are repeated 818 times to get full dataset.\n",
      "All (shuffled) inds are repeated 818 times to get full dataset.\n",
      "tensor([[0.7205, 0.7139, 0.7188,  ..., 0.7002, 0.6957, 0.6951],\n",
      "        [0.7139, 0.7205, 0.7202,  ..., 0.7157, 0.7259, 0.7094],\n",
      "        [0.7188, 0.7202, 0.7205,  ..., 0.6952, 0.6962, 0.6999],\n",
      "        ...,\n",
      "        [0.7002, 0.7157, 0.6952,  ..., 0.7205, 0.7323, 0.7135],\n",
      "        [0.6957, 0.7259, 0.6962,  ..., 0.7323, 0.7205, 0.7337],\n",
      "        [0.6951, 0.7094, 0.6999,  ..., 0.7135, 0.7337, 0.7205]]) tensor([[0.7205, 0.7139, 0.7188,  ..., 0.7002, 0.6957, 0.6951],\n",
      "        [0.7205, 0.7205, 0.7202,  ..., 0.7157, 0.7259, 0.7094],\n",
      "        [0.7205, 0.7205, 0.7205,  ..., 0.6952, 0.6962, 0.6999],\n",
      "        ...,\n",
      "        [0.7205, 0.7205, 0.7205,  ..., 0.7205, 0.7323, 0.7135],\n",
      "        [0.7205, 0.7205, 0.7205,  ..., 0.7205, 0.7205, 0.7337],\n",
      "        [0.7205, 0.7205, 0.7205,  ..., 0.7205, 0.7205, 0.7205]])\n",
      "40000 5 torch.Size([2]) torch.Size([2, 100])\n",
      "32000 2 torch.Size([28]) (tensor([ 147, 1457,  367,  751, 1486, 2499, 1700,  762, 2068, 1403,  584,   50,\n",
      "        1216, 1031,  567, 1279,  594,  195, 1650, 2437, 1154,   73,  485, 2071,\n",
      "        1787, 1873, 1546,  611], dtype=torch.int32), tensor([[ 0.7205,  0.6953,  0.6787,  ..., -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.6953,  0.7205,  0.7003,  ..., -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.6787,  0.7003,  0.7205,  ..., -1.0000, -1.0000, -1.0000],\n",
      "        ...,\n",
      "        [ 0.6685,  0.6773,  0.6608,  ..., -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.6122,  0.6241,  0.5901,  ..., -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.7037,  0.7026,  0.6956,  ..., -1.0000, -1.0000, -1.0000]]))\n"
     ]
    }
   ],
   "source": [
    "dsize = 40000\n",
    "vstart = int(dsize * 0.8)\n",
    "maxseqlen = 100\n",
    "maxind = 2502\n",
    "padval = -1\n",
    "batchsize = 150\n",
    "\n",
    "dataset = TokenizedKingDataset(datapath, filled=False, normalize=True, padval=padval, dsize=dsize, maxseqlen=maxseqlen, maxind=maxind, remove_starts=False)\n",
    "dataset_filled = TokenizedKingDataset(datapath, filled=True, normalize=True, padval=padval, dsize=dsize, maxseqlen=maxseqlen, maxind=maxind, remove_starts=False)\n",
    "\n",
    "# Don't need to random sample to make subsets since they're already pretty random, and want even distribution\n",
    "# of representative indices in both train/test\n",
    "train, test = Subset(dataset, range(vstart)), Subset(dataset, range(vstart, dsize))\n",
    "train_filled, test_filled = Subset(dataset_filled, range(vstart)), Subset(dataset_filled, range(vstart, dsize))\n",
    "\n",
    "dl_args = dict(batch_size=batchsize, shuffle=True, num_workers=6, collate_fn=TokenizedCollateFn(dataset.padind, dataset.padval).collate_fn)\n",
    "train_dataloader, test_dataloader = DataLoader(train, **dl_args), DataLoader(test, **dl_args)\n",
    "train_filled_dataloader, test_filled_dataloader = DataLoader(train_filled, **dl_args), DataLoader(test_filled, **dl_args)\n",
    "\n",
    "\n",
    "print(dataset_filled.X, dataset.X)\n",
    "print(len(dataset), len(dataset[-1][0]), dataset[0][0].shape, dataset[0][1].shape)\n",
    "print(len(train_filled), len(train_filled[0][0]), train_filled[2501][0].shape, train_filled[2501])\n",
    "# print(next(iter(test_filled_dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da448e-a25d-4852-9f91-8aaf3e8eaa76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T16:13:54.125685Z",
     "iopub.status.busy": "2024-07-05T16:13:54.125029Z",
     "iopub.status.idle": "2024-07-05T16:13:54.166989Z",
     "shell.execute_reply": "2024-07-05T16:13:54.166041Z",
     "shell.execute_reply.started": "2024-07-05T16:13:54.125656Z"
    }
   },
   "source": [
    "## Create Model(s)\n",
    "\n",
    "All use tokenized input (\"_t_\")\n",
    "\n",
    "35. Main model; one head; filled input\n",
    "35. 2 heads attention; filled input\n",
    "37. Non-filled input matrix; one head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dac57af-177f-4ff9-b301-ed4b1654f109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T04:37:49.853633Z",
     "iopub.status.busy": "2024-12-17T04:37:49.853299Z",
     "iopub.status.idle": "2024-12-17T04:37:49.971006Z",
     "shell.execute_reply": "2024-12-17T04:37:49.970109Z",
     "shell.execute_reply.started": "2024-12-17T04:37:49.853608Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TokenizedInputTransformer(\n",
      "  (pos_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=120, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=120, bias=True)\n",
      "        (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=120, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=120, bias=True)\n",
      "        (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed): Embedding(2504, 120, padding_idx=2503)\n",
      "  (outshape): Linear(in_features=120, out_features=100, bias=True)\n",
      "), TokenizedInputTransformer(\n",
      "  (pos_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=120, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=120, bias=True)\n",
      "        (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=120, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=120, bias=True)\n",
      "        (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed): Embedding(2504, 120, padding_idx=2503)\n",
      "  (outshape): Linear(in_features=120, out_features=100, bias=True)\n",
      "), TokenizedInputTransformer(\n",
      "  (pos_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=120, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=120, bias=True)\n",
      "        (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=120, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=120, bias=True)\n",
      "        (norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed): Embedding(2504, 120, padding_idx=2503)\n",
      "  (outshape): Linear(in_features=120, out_features=100, bias=True)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "# SMALL EMBEDDINGS!!!!\n",
    "d_model = 10\n",
    "\n",
    "model_names = [\"Test_35_t_f_1h\", \"Test_36_t_f_2h\", \"Test_37_t_1h\"]\n",
    "base_params = dict(d_model=d_model,\n",
    "                    num_encoder_layers=3,\n",
    "                    num_decoder_layers=2,\n",
    "                    dim_feedforward=512,\n",
    "                    activation=nn.Tanh(),\n",
    "                    use_pe=True,\n",
    "                    dropout_pe=0.0,\n",
    "                    maxseqlen=maxseqlen, \n",
    "                    maxind=maxind\n",
    "                  )\n",
    "\n",
    "run_details = {\"run_params\": dict(\n",
    "                    machine=machine,\n",
    "                    epochs = 250,\n",
    "                    checkpoint_at = 50,\n",
    "                    load=True,\n",
    "                    batch_pr=int(dsize / batchsize / 5), # Print/validate every 1/5 of epoch\n",
    "                    runname=runname\n",
    "                    ),\n",
    "                model_names[0]: dict(\n",
    "                    num_head=1,\n",
    "                    name=model_names[0],\n",
    "                    ) | base_params,\n",
    "                model_names[1]: dict(\n",
    "                    num_head=2,\n",
    "                    name=model_names[1],\n",
    "                    ) | base_params,\n",
    "                model_names[2]: dict(\n",
    "                    num_head=1,\n",
    "                    name=model_names[2],\n",
    "                    ) | base_params,\n",
    "                }\n",
    "models = [TokenizedInputTransformer(**run_details[m]).to(device) for m in model_names]\n",
    "\n",
    "assert models[0].padind == dataset_filled.padind\n",
    "\n",
    "print(models)\n",
    "\n",
    "# Save details\n",
    "with open(os.path.join(outdir, f\"details_{runname}.json\"), \"w\" ) as write:\n",
    "    json.dump(run_details, write, indent=2, default=lambda x: f\"nn.{x.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4478fd-aaa8-4fb9-a30c-1d28a4991d5e",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1561b9e0-e90e-4f37-a748-32a69f34565c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T19:09:42.573310Z",
     "iopub.status.busy": "2024-11-26T19:09:42.572966Z",
     "iopub.status.idle": "2024-11-26T19:10:04.437295Z",
     "shell.execute_reply": "2024-11-26T19:10:04.436469Z",
     "shell.execute_reply.started": "2024-11-26T19:09:42.573283Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 98, 100]) tensor([[[ 0.5424, -0.5737,  0.3865,  ...,  0.3032, -0.1889, -0.0847],\n",
      "         [ 0.2757,  0.7767, -0.7470,  ...,  0.9186,  0.5960,  0.9575],\n",
      "         [-0.4169, -0.8034,  0.1595,  ...,  0.5477, -0.4676,  0.0498],\n",
      "         ...,\n",
      "         [-0.6248,  0.0357,  0.1504,  ...,  1.4623,  0.3608,  0.1969],\n",
      "         [-0.0325, -0.0228,  0.3965,  ...,  0.9481,  0.1958, -0.7405],\n",
      "         [-0.0075,  0.1558,  0.2931,  ...,  1.7159,  0.1304, -0.2127]],\n",
      "\n",
      "        [[ 0.2671,  0.4824,  0.1180,  ..., -0.0022, -1.0225,  0.2591],\n",
      "         [ 0.1399,  0.2514, -0.0843,  ...,  1.0780, -0.2568,  0.6659],\n",
      "         [-0.4793, -0.0486, -0.1473,  ...,  0.2168,  0.1735, -0.0826],\n",
      "         ...,\n",
      "         [-0.4887, -0.3468, -0.7329,  ...,  0.5380, -0.2615,  0.4388],\n",
      "         [ 0.6219, -0.4193, -0.5936,  ...,  0.0726, -0.0848,  0.2610],\n",
      "         [ 0.3732, -0.1915, -1.0530,  ...,  0.0859,  0.1695,  0.5922]],\n",
      "\n",
      "        [[-0.4227,  0.8049,  0.6250,  ...,  1.0032, -0.1554,  0.0846],\n",
      "         [ 0.3641, -0.4829,  0.3436,  ...,  1.3082, -0.1914,  0.0767],\n",
      "         [ 1.0076, -0.5795, -0.2522,  ...,  0.3233, -0.0880,  0.5701],\n",
      "         ...,\n",
      "         [-1.1196, -0.1967,  0.5955,  ...,  0.5298, -0.4539, -0.3784],\n",
      "         [-0.9630, -0.0896,  0.6366,  ...,  1.1980, -0.8097,  0.1123],\n",
      "         [-0.9907, -0.2085,  1.0569,  ...,  0.7232, -0.3940, -0.0900]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0199,  0.0839, -1.1090,  ...,  1.0817, -1.2052, -0.0578],\n",
      "         [ 0.0403, -0.7756, -0.2161,  ...,  0.7628, -0.3806, -0.7818],\n",
      "         [ 0.9262, -0.7097, -0.7203,  ...,  1.9353, -0.4236, -0.5223],\n",
      "         ...,\n",
      "         [ 0.5874, -1.0666, -0.1667,  ...,  1.6554,  0.2654,  0.1342],\n",
      "         [ 0.6579, -0.6781, -0.4309,  ...,  1.1902,  0.2460, -0.2975],\n",
      "         [ 0.4084, -0.5982, -0.6372,  ...,  1.1824, -0.0370,  0.3961]],\n",
      "\n",
      "        [[ 0.3819, -0.2748,  0.3964,  ...,  0.3057,  0.5104,  0.3905],\n",
      "         [ 0.4658,  0.2472, -0.4277,  ...,  0.8493, -0.3016,  0.0745],\n",
      "         [ 0.0427, -0.1574, -0.3107,  ...,  0.8785, -0.1919, -0.0562],\n",
      "         ...,\n",
      "         [ 0.0513, -0.3506,  1.2616,  ...,  1.1596,  0.6361, -0.7288],\n",
      "         [-0.1151, -0.2615,  1.7165,  ...,  1.0144,  0.8913, -1.2939],\n",
      "         [ 0.3816, -0.2488,  0.9854,  ...,  0.8348,  1.1564, -0.8471]],\n",
      "\n",
      "        [[ 0.9138, -0.1803,  1.1686,  ...,  0.5125, -0.5417, -0.2003],\n",
      "         [ 0.4566,  0.4457,  0.5693,  ...,  0.9592,  0.2104, -1.3583],\n",
      "         [ 1.0451, -0.3963,  0.3979,  ...,  0.0838,  0.1921, -1.0932],\n",
      "         ...,\n",
      "         [ 0.5378, -0.1729,  1.1960,  ...,  0.9098,  0.4770, -0.1971],\n",
      "         [ 0.5972, -0.1206,  0.9463,  ...,  0.7154,  0.2992, -0.1298],\n",
      "         [ 0.7800, -0.2877,  0.8798,  ...,  0.5862,  0.5116, -0.2069]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# tstdata = next(iter(train_filled_dataloader))\n",
    "# f = models[0](tstdata[0], tstdata[2])\n",
    "# print(f.shape, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88784850-d7ec-4cc8-bf32-af288e03b966",
   "metadata": {},
   "source": [
    "## Train the Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "760e2659-92f6-4921-893f-80090f91b240",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-17T04:37:53.631383Z",
     "iopub.status.busy": "2024-12-17T04:37:53.631045Z",
     "iopub.status.idle": "2024-12-17T04:41:30.615496Z",
     "shell.execute_reply": "2024-12-17T04:41:30.613745Z",
     "shell.execute_reply.started": "2024-12-17T04:37:53.631355Z"
    },
    "gather": {
     "logged": 1723055543098
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Test_32_t_f_1h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bjonnalagadda/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 53] loss: 0.09224236686274691, validation loss: 0.004492702721445648, average train time (sec): 0.02510899011319435\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Set foreach=False to avoid OOM\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0003\u001b[39m, foreach\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_model_tokenized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss_fcn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fcn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_filled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_run_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_onnx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UCB-O365/Work/Ecotone/Internship2024/src/lib/training.py:92\u001b[0m, in \u001b[0;36mtrain_model_tokenized\u001b[0;34m(model, optimizer, train_data, validate_data, output_run_dir, machine, loss_fcn, padval, epochs, checkpoint_at, load, batch_pr, writer, output_onnx, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m out_seq \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Loss function\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UCB-O365/Work/Ecotone/Internship2024/src/lib/models.py:212\u001b[0m, in \u001b[0;36mTokenizedInputTransformer.forward\u001b[0;34m(self, x, key_padding_mask, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(src)\n\u001b[1;32m    209\u001b[0m output_encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x,\n\u001b[1;32m    210\u001b[0m                               src_key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    211\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 212\u001b[0m output_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moutput_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutshape(output_decoder)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/transformer.py:369\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 369\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/transformer.py:718\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    716\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[1;32m    717\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[0;32m--> 718\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/transformer.py:744\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 744\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout3(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "\n",
    "loss_fcn = tokenized_masked_loss\n",
    "\n",
    "for model, train_d, test_d in zip(models, \n",
    "                                  (train_filled_dataloader, train_filled_dataloader, train_dataloader), \n",
    "                                  (test_filled_dataloader, test_filled_dataloader, test_dataloader)):\n",
    "\n",
    "    writer = SummaryWriter(os.path.join(tensorboard_dir, f'{machine}_{model.get_name()}_{runname}'))\n",
    "    # Set foreach=False to avoid OOM\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, foreach=False)\n",
    "    \n",
    "    train_model_tokenized(model=model,\n",
    "                optimizer=optimizer,\n",
    "                train_data=train_d,\n",
    "                validate_data=test_d,\n",
    "                loss_fcn=loss_fcn,\n",
    "                padval=dataset_filled.padval,\n",
    "                output_run_dir=outdir,\n",
    "                **run_details[\"run_params\"],\n",
    "                writer=writer,\n",
    "                output_onnx=False\n",
    "            )\n",
    "    \n",
    "    \n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e6ae33-e865-4c26-8e88-40881ec2d57a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
